# Gesture Control System ğŸ¥ğŸ–±ï¸âœ‹ğŸ®ğŸ¨

A **multi-functional gesture recognition system** that uses computer vision, hand-tracking, and AI-powered tools to control media, presentations, mouse, games, and even generate AI art â€” **all with your hands**.  

The project integrates **OpenCV, MediaPipe, PyQt6, PyAutoGUI, Pycaw, Pygame, and Hugging Face APIs** to provide an interactive gesture-controlled experience.

---

## ğŸš€ Features

- **Movie & Presentation Mode**
  - Play/Pause media with a fist âœŠ  
  - Control volume with a pinch ğŸ¤  
  - Slide navigation with left/right index finger ğŸ‘‰ğŸ‘ˆ  

- **Mouse Control Mode**
  - Control the cursor with your index finger  
  - Left click (Thumbâ€“Index pinch)  
  - Right click (Thumbâ€“Middle pinch)  

- **Testing Modes**
  - **Hand Detection** â†’ Visualize hand landmarks  
  - **Gesture Detection** â†’ Finger counting, peace âœŒï¸, thumbs up ğŸ‘, etc.  

- **Air Drawing & Games**
  - **Air Canvas** â†’ Draw in the air, fist to clear  
  - **Flappy Bird Game** â†’ Control the bird with an open palm âœ‹  
  - **AI Air Canvas** â†’ Draw â†’ Pinch â†’ Convert sketch to **AI-generated art**  

---

## ğŸ“‚ Project Structure

```
Gesture-Control-System/
â”‚â”€â”€ project.py            # Main application script
â”‚â”€â”€ Images/               # Instruction images (e.g., S5.png, S6.png, etc.)
â”‚â”€â”€ README.md             # Documentation (this file)
```

---

## âš™ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/<your-username>/Gesture-Control-System.git
   cd Gesture-Control-System
   ```

2. **Create a virtual environment (recommended)**
   ```bash
   python -m venv venv
   source venv/bin/activate   # Mac/Linux
   venv\Scripts\activate      # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

   Or install manually:
   ```bash
   pip install opencv-python mediapipe pyqt6 pyautogui pygame numpy pillow transformers huggingface_hub pycaw comtypes
   ```

   > âš ï¸ `pycaw` works only on Windows for volume control.

---

## â–¶ï¸ Usage

1. **Launch GUI**
   ```bash
   python project.py
   ```

   A window will appear with all available modes.

2. **Run specific mode directly**
   ```bash
   python project.py --mode session5       # Play/Pause control
   python project.py --mode session6       # Volume control
   python project.py --mode session7       # Slide/Video navigation
   python project.py --mode session8       # Mouse control
   python project.py --mode session9       # Air Canvas
   python project.py --mode session10      # Flappy Bird
   python project.py --mode ai_air_canvas  # AI-powered Air Canvas
   ```

---

## ğŸ“¸ Screenshots

Screenshots of each mode are available inside the **Images/** folder.  
The GUI also displays these images alongside instructions when you launch a mode.  

---

## ğŸ”‘ Requirements

- Python **3.8+**
- Webcam for gesture recognition  
- Windows (for full feature support, esp. `pycaw` volume control)  
- GPU recommended for faster AI image generation  

---

## ğŸ§  AI-Powered Features

- Uses **Hugging Face API** + **Stable Diffusion XL** for converting air sketches into AI-generated art.  
- Requires a Hugging Face API key â€” replace the placeholder key in `project.py` with your own.  

```python
client = InferenceClient(provider="nscale", api_key="your_api_key_here")
```

Get a free key from: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)  

---

## ğŸ¤ Contributing

1. Fork the project  
2. Create a new branch (`feature-newmode`)  
3. Commit your changes  
4. Push to the branch  
5. Open a Pull Request  

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€“ feel free to use, modify, and share.  
